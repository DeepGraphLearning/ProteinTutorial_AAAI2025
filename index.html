<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Artificial Intelligence for Protein Design, AAAI 2025 Tutorial</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="icon" type="image/png" href="/images/favicon-32x32.svg">
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&display=swap" rel="stylesheet">
  <link href="https://deepgraphlearning.github.io/ProteinTutorial_AAAI2025/assets/css/style.css" rel="stylesheet">
  
  <meta property="og:title" content="" />
  <meta property="og:type" content="" />
  <meta property="og:url" content="" />
  <meta property="og:image" content="" />
  <meta property="og:description" content="" />
</head>
<body class='page '>
  <div id="main-menu-mobile" class="main-menu-mobile">
  <ul>
    
  </ul>
</div>

  <div id="wrapper" class="wrapper">
    
    <div class="container pb-6 pt-6 pt-md-10 pb-md-10">
  <div class="row justify-content-start">
    <div class="col-12 col-md-12">
      <h1 class="title">AAAI 2025 Tutorial<br>Artificial Intelligence for Protein Design</h1>
      
<div class="row">

  <div class="col-12 col-md-4 mb-2">
    <div class="team team-summary team-summary-large">
      
      <div class="team-image">
        <img alt="Zuobai Zhang" class="img-fluid mb-2" src="https://deepgraphlearning.github.io/ProteinTutorial_AAAI2025/assets/images/team/zuobai_zhang.JPG" />
      </div>
      
      <div class="team-meta">
        <h2 class="team-name"><a href="https://https://oxer11.github.io//">Zuobai Zhang</a></h2>
        <p class="team-description">Mila - Quebec AI Institute</p>
      </div>
    </div>
  </div>

  <div class="col-12 col-md-4 mb-2">
    <div class="team team-summary team-summary-large">
      
      <div class="team-image">
        <img alt="Jiarui Lu" class="img-fluid mb-2" src="https://deepgraphlearning.github.io/ProteinTutorial_AAAI2025/assets/images/team/jiarui_lu.JPG" />
      </div>
      
      <div class="team-meta">
        <h2 class="team-name"><a href="https://lujiarui.github.io/">Jiarui Lu</a></h2>
        <p class="team-description">Mila - Quebec AI Institute</p>
      </div>
    </div>
  </div>

  <div class="col-12 col-md-4 mb-2">
    <div class="team team-summary team-summary-large">
      
      <div class="team-image">
        <img alt="Divya Nori" class="img-fluid mb-2" src="https://deepgraphlearning.github.io/ProteinTutorial_AAAI2025/assets/images/team/divya_nori.png" />
      </div>
      
      <div class="team-meta">
        <h2 class="team-name"><a href="https://divnori.github.io/">Divya Nori</a></h2>
        <p class="team-description">MIT</p>
      </div>
    </div>
  </div>

  <div class="col-12 col-md-4 mb-2">
    <div class="team team-summary team-summary-large">
      
      <div class="team-image">
        <img alt="Jiwoong Park" class="img-fluid mb-2" src="https://deepgraphlearning.github.io/ProteinTutorial_AAAI2025/assets/images/team/jiwoong_park.jpg" />
      </div>
      
      <div class="team-meta">
        <h2 class="team-name"><a href="https://jiwoongpark92.github.io/">Jiwoong Park</a></h2>
        <p class="team-description">Northeastern University</p>
      </div>
    </div>
  </div>

  <div class="col-12 col-md-4 mb-2">
    <div class="team team-summary team-summary-large">
      
      <div class="team-image">
        <img alt="Wengong Jin" class="img-fluid mb-2" src="https://deepgraphlearning.github.io/ProteinTutorial_AAAI2025/assets/images/team/wengong_jin.png" />
      </div>
      
      <div class="team-meta">
        <h2 class="team-name"><a href="https://wengong-jin.github.io/">Wengong Jin</a></h2>
        <p class="team-description">Northeastern University</p>
      </div>
    </div>
  </div>
  
  <div class="col-12 col-md-4 mb-2">
    <div class="team team-summary team-summary-large">
      
      <div class="team-image">
        <img alt="Jian Tang" class="img-fluid mb-2" src="https://deepgraphlearning.github.io/ProteinTutorial_AAAI2025/assets/images/team/jian_tang.jpg" />
      </div>
      
      <div class="team-meta">
        <h2 class="team-name"><a href="https://jian-tang.com/">Jian Tang</a></h2>
        <p class="team-description">Mila - Quebec AI Institute</p>
      </div>
    </div>
  </div>
  
</div>

<h2 id="abstract">Abstract</h2>
<p>Proteins are fundamental to biological processes, and AI techniques are revolutionizing their study, with applications ranging from drug 
  discovery to enzyme design. A key challenge in protein science is to predict and design protein sequences and structures, and to model 
  their dynamics. In this tutorial, we will present a comprehensive overview of AI approaches applied to protein sequence, structure, and 
  function prediction and design. Topics include sequence-based and structure-based protein representation learning, protein folding and 
  dynamics prediction, and protein design with generative models. Participants are expected to have a foundational understanding of machine 
  learning methods (e.g., neural networks, generative models). No prior experience with computational biology or bioinformatics is necessary, 
  as the tutorial will include a comprehensive introduction to the field.</p>

<h2 id="schedule">Schedule</h2>
<p>8:30 am - 12:30 pm EST, February 26, 2025</p>

<h2 id="schedule">Location</h2>
<p>Room 117, Philadelphia Convention Center, Philadelphia, PA USA</p>

<h2 id="slides">Slides</h2>
<p>The slides can be found <a href="TBA">here</a>.</p>

<h2 id="outline">Outline</h2>

<ul>
  <li><strong>Part I: Introduction [30 min, Wengong]</strong>
    <ul>
      <li>Protein 101: Understanding and Modeling</li>
      <li>Learning on Protein Data</li>
      <li>Methodology Overview</li>
      <li><strong>@Wengong: To be updated</strong></li>
    </ul>
  </li>
  <li><strong>Part II: Protein Representation Learning [60 min, Zuobai, <a href="https://drive.google.com/file/d/147luOEAlaQUOC97DcPoG3FoZRVGzhhuq/view?usp=sharing">slides</a>]</strong>
    <ul>
      <li>Sequence Representation Learning
        <ul>
          <li>Autoregressive Language Model [ProGen <a class="citation" href="#madani2023progen">(Madani et al., 2023)</a>]</li>
          <li>Masked Language Model [ESM-1 <a class="citation" href="#rives2021esm">(Rives et al., 2021)</a>, ESM-2 <a class="citation" href="#lin2023esm2">(Lin et al., 2023)</a>, ProtTrans <a class="citation" href="#elnaggar2021prottrans">(Elnaggar et al., 2021)</a>]</li>
          <li>Diffusion Language Model [DPLM <a class="citation" href="#wang2024dplm">(Wang et al., 2024)</a>]</li>
        </ul>
      </li>
      <li>Structure Representation Learning
        <ul>
          <li>Geometric Deep Learning [EGNN <a class="citation" href="#satorras2021egnn">(Satorras et al., 2021)</a>]</li>
          <li>Protein Structure Encoder [GVP <a class="citation" href="#jing2021gvp">(Jing et al., 2021)</a>, IEConv <a class="citation" href="#hermosilla2021ieconv">(Hermosilla et al., 2021)</a>, GearNet <a class="citation" href="#zhang2023gearnet">(Zhang et al., 2023)</a>, ProNet <a class="citation" href="#wang2023pronet">(Wang et al., 2023)</a>, CDConv <a class="citation" href="#fan2023cdconv">(Fan et al., 2023)</a>]
          </li>
          <li>Structure Pre-Training Algorithm [GearNet <a class="citation" href="#zhang2023gearnet">(Zhang et al., 2023)</a>, SiamDiff <a class="citation" href="#zhang2023siamdiff">(Zhang et al., 2023)</a>]
          </li>
        </ul>
      </li>
      <li>Multi-Modality Representation Learning
        <ul>
          <li>Sequence + Structure [LM-GVP <a class="citation" href="#wang2023lmgvp">(Wang et al., 2023)</a>, ESM-GearNet <a class="citation" href="#zhang2023esmgearnet">(Zhang et al., 2023)</a>, SaProt <a class="citation" href="#su2023saprot">(Su et al., 2023)</a>, ProstT5 <a class="citation" href="#heinzinger2023prostt5">(Heinzinger et al., 2024)</a>, DPLM-2 <a class="citation" href="#wang2024dplm2">(Wang et al., 2024)</a>]</li>
          <li>Sequence + Structure + Function [ESM3 <a class="citation" href="#hayes2024esm3">(Hayes et al., 2024)</a>]</li>
          <li>Sequence + Text [OntoProtein <a class="citation" href="#zhang2023ontoprotein">(Zhang et al., 2022)</a>, ProtST <a class="citation" href="#xu2023protst">(Xu et al., 2023)</a>]</li>
        </ul>
      </li>
      <li>Application
        <ul>
          <li>Protein Understanding Tasks [PEER <a class="citation" href="#xu2022peer">(Xu et al., 2022)</a>]</li>
          <li>Protein Fitness Prediction [ProteinGym <a class="citation" href="#notin2023proteingym">(Notin et al., 2023)</a>, S3F <a class="citation" href="#zhang2024s3f">(Zhang et al., 2024)</a>]</li>
          <li>Antibody Affinity Optimization [BindDDG <a class="citation" href="#shan2022bindddg">(Shan et al., 2022)</a>, GearBind <a class="citation" href="#cai2024gearbind">(Cai et al., 2024)</a>]</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Break: 15 min</strong></li>
  <li><strong>Part III: Protein Structure and Dynamics Prediction [60 min, Jiarui]</strong>
    <ul>
  <li><strong>Protein Structure Prediction</strong>
    <ul>
      <li>Single-chain Folding: AlphaFold2 <a class="citation" href="#jumper2021alphafold">(Jumper et al., 2021)</a>, RoseTTAFold <a class="citation" href="#baek2021rosettafold">(Baek et al., 2021)</a>, OmegaFold <a class="citation" href="#wu2022omegafold">(Wu et al., 2022)</a>, ESMFold <a class="citation" href="#lin2023esmfold">(Lin et al., 2023)</a></li>
      <li>Side-chain Packing: AttnPacker <a class="citation" href="#mcpartlon2023attnpacker">(McPartlon et al., 2023)</a>, DiffPack <a class="citation" href="#zhang2024diffpack">(Zhang et al., 2024)</a></li>
      <li>Complex Prediction: AlphaFold-Multimer <a class="citation" href="#evans2021alphafoldmultimer">(Evans et al., 2021)</a>, RoseTTAFold-AA <a class="citation" href="#krishna2024rosettafoldaa">(Krishna et al., 2024)</a>, Umol <a class="citation" href="#bryant2024umol">(Bryant et al., 2024)</a>, AlphaFold3 <a class="citation" href="#abramson2024alphafold3">(Abramson et al., 2024)</a></li>
    </ul>
  </li>
  <li><strong>Protein Conformation Sampling</strong>
    <ul>
      <li>Boltzmann Generators: <a class="citation" href="#noe2019boltzmann">Noé et al., 2019</a></li>
      <li>Coarse-Graining Based: Two for One <a class="citation" href="#arts2023twoforone">(Arts et al., 2023)</a>, EigenFold <a class="citation" href="#jing2023eigenfold">(Jing et al., 2023)</a></li>
      <li>Rigid-Frame Based: Str2Str <a class="citation" href="#lu2024str2str">(Lu et al., 2024)</a>, ConfDiff <a class="citation" href="#wang2024confdiff">(Wang et al., 2024)</a>, DiG <a class="citation" href="#zheng2024dig">(Zheng et al., 2024)</a>, AlphaFlow <a class="citation" href="#jing2024alphaflow">(Jing et al., 2024)</a>, BioEmu <a class="citation" href="#lewis2024bioemu">(Lewis et al., 2024)</a></li>
      <li>Structure Language Models: ESMDiff <a class="citation" href="#lu2025esmdiff">(Lu et al., 2025)</a></li>
    </ul>
  </li>
  <li><strong>MD Trajectory Emulation</strong>
    <ul>
      <li>Neural Simulator: CGMD <a class="citation" href="#fu2023cgmd">(Fu et al., 2023)</a></li>
      <li>Conditional Transfer Operator: ITO <a class="citation" href="#schreiner2023ito">(Schreiner et al., 2023)</a>, TimeWarp <a class="citation" href="#klein2023timewarp">(Klein et al., 2023)</a></li>
      <li>Trajectory Generator: MDGen <a class="citation" href="#jing2024mdgen">(Jing et al., 2024)</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Part IV: Protein Design [60 min, Wengong]</strong>
    <ul>
      <li>Sequence Design
        <ul>
          <li>ProGen [], ProGen-2 []</li>
          <li>ESM-IF [], ProteinMPNN []</li>
        </ul>
      </li>
      <li>Structure Design
        <ul>
          <li>FrameDiff [], FrameFlow []</li>
          <li>Genie [], Genie2 []</li>
          <li>Chroma []</li>
          <li>RFDiffusion []</li>
          <li>FoldFlow [], FoldFlow-2 []</li>
        </ul>
      </li>
      <li>Sequence-Structure Co-Design
        <ul>
          <li>ProtSeed [], ProteinGenerator [], MultiFlow [], Protpardelle [], DPLM-2 [</li>
        </ul>
      </li>
      <li>Antibody Design [RefineGNN (Jin et al., 2021)]</li>
      <li><strong>@Wengong: To be updated</strong></li>
    </ul>
  </li>
  <li><strong>Part V: Concluding Remarks and Future Works [15 min, Jian]</strong>
    <ul>
      <li>QA [5 min]</li>
    </ul>
  </li>
</ul>


<h2 id="organizers">Organizers</h2>

<ul>
  <li>Zuobai Zhang, <a href="https:/oxer11.github.io/">website</a><br />
    <ul>
      <li>Zuobai Zhang is a 4th-year Ph.D. student at Mila – Québec AI Institute, advised by Prof. Jian Tang. He obtained B.Sc. in computer science from Fudan University. Previously, he interned at the Fundamental GenAI team at NVResearch. His research focuses on developing protein structure foundation models.</li>
      <!-- <li><strong>Relevant reviewing experience.</strong>  Jian Tang has served as a reviewer at the major conferences of machine learning, data mining, and natural language processing communities including NIPS, ICML, ICLR, AAAI, IJCAI, ACL, EMNLP, KDD, WWW, and WSDM.</li> -->
    </ul>
  </li>
  <li>Jiarui Lu, <a href="https://lujiarui.github.io/">website</a><br />
    <ul>
      <li>Jiarui Lu is a 3rd-year Ph.D. student at Mila - Québec AI Institute, supervised by Prof. Jian Tang. He obtained B.Sc. in chemistry and mathematics from Shanghai Jiao Tong University. His research focuses on generative learning on biomolecular structure data such as proteins.  
      </li>
      </ul>
  </li>
  <li>Divya Nori, <a href="https://divnori.github.io/">website</a><br />
    <ul>
      <li>Divya Nori is a Senior and joint Master’s student at MIT  and student researcher at the Broad Institute, advised by Prof. Wengong Jin and Prof. Caroline Uhler. Previously, she interned on the ML teams at D.E. Shaw Research, Absci, and Microsoft Research. Her research focuses on developing AI methods for biomolecular design.
      </li>
      </ul>
  </li>
  <li>Jiwoong Park, <a href="https://jiwoongpark92.github.io/">website</a><br />
    <ul>
      <li>Jiwoong Park is a postdoctoral researcher at Northeastern University working with Professor Wengong Jin. He completed his PhD in electrical and computer engineering at Seoul National University. His research field is generative models for drug design and machine learning for graph-structured data.
      </li>    
    </ul>
  </li>
  <li>Wengong Jin, <a href="https://wengong-jin.github.io/">website</a><br />
    <ul>
      <li>Wengong Jin is an assistant professor at Khoury College of Computer Sciences at Northeastern University. His research focuses on geometric and generative AI models for drug discovery. His work has been published in journals including ICML, NeurIPS, ICLR, Nature, Science, Cell, and PNAS, and covered by such outlets as the Guardian, BBC News, and CBS Boston.
      </li>
    </ul>
  </li>
  <li>Jian Tang, <a href="https://jian-tang.com/">website</a><br />
    <ul>
      <li>Jian Tang is an associate professor at Mila - Québec AI Institute, a Canada CIFAR AI Research Chair and the founder and CEO of BioGeometry. His research interests are deep generative models, graph machine learning and their applications to drug discovery. He has done many pioneering work on AI for drug discovery, including the first open-source machine learning framework for drug discovery, TorchDrug and TorchProtein.
      </li>
    </ul>
  </li>
</ul>

<h2 id="references">References</h2>
<ol class="bibliography">
<li><span id="bepler2019plm">Bepler, Tristan, Berger, Bonnie. "Learning the protein language: Evolution, structure, and function."" <i>Cell System 2019</i>.</span></li>
<li><span id="rao2019tape">Rao, Roshan, et al. "Evaluating protein transfer learning with TAPE." <i>NeurIPS 2019</i>.</span></li>
<li><span id="madani2023progen">Madani, Ali, et al. "Large language models generate functional protein sequences across diverse families."" <i>Nature Biotechnology 2023</i>.</span></li>
<li><span id="rives2021esm">Rives, Alexander, et al. "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences." <i>PNAS 2021</i>.</span></li>
<li><span id="lin2023esm2">Lin, Zeming, et al. "Evolutionary-scale prediction of atomic-level protein structure with a language model." <i>Science 2023</i>.</span></li>
<li><span id="wang2024dplm">Wang, Xinyou, et al. "Diffusion Language Models Are Versatile Protein Learners." <i>ICML 2024</i>.</span></li>
<li><span id="rao2021msat">Rao, Roshan, et al. "MSA Transformer."" <i>ICML 2021</i>.</span></li>
<li><span id="elnaggar2021prottrans">Elnaggar, Ahmed, et al. "Prottrans: Toward understanding the language of life through self-supervised learning." <i>TPAMI 2021</i>.</span></li>
<li><span id="satorras2021egnn">Satorras, Victor Garcia, Emiel Hoogeboom, and Max Welling. "E(n) equivariant graph neural networks." <i>ICML 2021</i>.</span></li>
<li><span id="jing2021gvp">Jing, Bowen, et al. "Learning from protein structure with geometric vector perceptrons." <i>ICLR 2021</i>.</span></li>
<li><span id="hermosilla2021ieconv">Hermosilla, Pedro, et al. "Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures." <i>ICLR 2021</i>.</span></li>
<li><span id="zhang2023gearnet">Zhang, Zuobai, et al. "Protein representation learning by geometric structure pretraining." <i>ICLR 2023</i>.</span></li>
<li><span id="wang2023pronet">Wang, Limei, et al. "Learning Hierarchical Protein Representations via Complete 3D Graph Networks." <i>ICLR 2023</i>.</span></li>
<li><span id="fan2023cdconv">Fan, Hehe, et al. "Continuous-discrete convolution for geometry-sequence modeling in proteins." <i>ICLR 2023</i>.</span></li>
<li><span id="chen2023pretrain">Chen, Can, et al. "Structure-aware protein self-supervised learning." <i>Bioinformatics 2023</i>.</span></li>
<li><span id="zhang2023siamdiff">Zhang, Zuobai, et al. "Pre-training protein encoder via siamese sequence-structure diffusion trajectory prediction." <i>NeurIPS 2023</i>.</span></li>
<li><span id="wang2023lmgvp">Wang, Zichen, et al. "LM-GVP: an extensible sequence and structure informed deep learning framework for protein property prediction." <i>Scientific Reports 2023</i>.</span></li>
<li><span id="zhang2023esmgearnet">Zhang, Zuobai, et al. "A systematic study of joint representation learning on protein sequences and structures." <i>ArXiv Preprint ArXiv:2303.06275</i>.</span></li>
<li><span id="su2023saprot">Su, Jin, et al. "Saprot: Protein language modeling with structure-aware vocabulary." <i>ICLR 2024</i>.</span></li>
<li><span id="heinzinger2024prostt5">Heinzinger, Michael, et al. "Bilingual Language Model for Protein Sequence and Structure." <i>NAR Genomics and Bioinformatics 2024</i>.</span></li>
<li><span id="wang2024dplm2">Wang, Xinyou, et al. "DPLM-2: A multimodal diffusion protein language model." <i>ICLR 2025</i>.</span></li>
<li><span id="hayes2024esm3">Hayes, Thomas, et al. "Simulating 500 million years of evolution with a language model." <i>Science 2024</i>.</span></li>
<li><span id="xu2023protst">Xu, Minghao, et al. "ProtST: Multi-modality learning of protein sequences and biomedical texts." <i>ICML 2023</i>.</span></li>
<li><span id="zhang2022ontoprotein">Zhang, Ningyu, et al. "OntoProtein: Protein Pretraining With Gene Ontology Embedding." <i>ICLR 2022</i>.</span></li>
<li><span id="xu2022peer">Xu, Minghao, et al. "PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding." <i>NeurIPS DB Track 2022</i>.</span></li>
<li><span id="zhu2022torchdrug">Zhu, Zhaocheng, et al. "TorchDrug: A Powerful and Flexible Machine Learning Platform for Drug Discovery." <i>ArXiv Preprint ArXiv:2202.08320</i>.</span></li>
<li><span id="notin2023proteingym">Notin, Pascal, et al. "ProteinGym: Large-Scale Benchmarks for Protein Design and Fitness Prediction." <i>NeurIPS 2023</i>.</span></li>
<li><span id="zhang2024s3f">Zhang, Zuobai, et al. "Multi-Scale Representation Learning for Protein Fitness Prediction." <i>NeurIPS 2024</i>.</span></li>
<li><span id="shan2022bindddg">Shan, Sisi, et al. "Deep Learning-Guided Optimization of Human Antibody Against SARS-CoV-2 Variants with Broad Neutralization." <i>PNAS 2022</i>.</span></li>
<li><span id="cai2024gearbind">Cai, Huiyu, et al. "Pretrainable Geometric Graph Neural Network for Antibody Affinity Maturation." <i>Nature Communications 2024</i>.</span></li>

<li><span id="jumper2021alphafold">Jumper, John, et al. "Highly accurate protein structure prediction with AlphaFold." <i>Nature 2021</i>.</span></li>
<li><span id="baek2021rosettafold">Baek, Minkyung, et al. "Accurate prediction of protein structures and interactions using a three-track neural network." <i>Science 2021</i>.</span></li>
<li><span id="wu2022omegafold">Wu, Ruidong, et al. "High-resolution de novo structure prediction from primary sequence." <i>BioRxiv 2022</i>.</span></li>
<li><span id="lin2023esmfold">Lin, Zeming, et al. "Evolutionary-scale prediction of atomic-level protein structure with a language model." <i>Science 2023</i>.</span></li>
<li><span id="mcpartlon2023attnpacker">McPartlon, Matthew, et al. "An end-to-end deep learning method for protein side-chain packing and inverse folding." <i>PNAS 2023</i>.</span></li>
<li><span id="zhang2024diffpack">Zhang, Yangtian, et al. "Diffpack: A torsional diffusion model for autoregressive protein side-chain packing." <i>NeurIPS 2024</i>.</span></li>
<li><span id="evans2021alphafoldmultimer">Evans, Richard, et al. "Protein complex prediction with AlphaFold-Multimer." <i>bioRxiv 2021</i>.</span></li>
<li><span id="krishna2024rosettafoldaa">Krishna, Rohith, et al. "Generalized biomolecular modeling and design with RoseTTAFold All-Atom." <i>Science 2024</i>.</span></li>
<li><span id="bryant2024umol">Bryant, Patrick, et al. "Structure prediction of protein-ligand complexes from sequence information with Umol." <i>Nature Communications 2024</i>.</span></li>
<li><span id="abramson2024alphafold3">Abramson, Josh, et al. "Accurate structure prediction of biomolecular interactions with AlphaFold 3." <i>Nature 2024</i>.</span></li>
<li><span id="noe2019boltzmann">Noé, Frank, et al. "Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning." <i>Science 2019</i>.</span></li>
<li><span id="arts2023twoforone">Arts, Marloes, et al. "Two for one: Diffusion models and force fields for coarse-grained molecular dynamics." <i>JCTC 2023</i>.</span></li>
<li><span id="jing2023eigenfold">Jing, Bowen, et al. "Eigenfold: Generative protein structure prediction with diffusion models." <i>ICLR 2023 MLDD</i>.</span></li>
<li><span id="lu2024str2str">Lu, Jiarui, et al. "Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling." <i>The Twelfth International Conference on Learning Representations (ICLR 2024)</i>.</span></li>
<li><span id="wang2024confdiff">Wang, Yan, et al. "Protein conformation generation via force-guided se (3) diffusion models." <i>The 41st International Conference on Machine Learning (ICML 2024)</i>.</span></li>
<li><span id="zheng2024dig">Zheng, Shuxin, et al. "Predicting equilibrium distributions for molecular systems with deep learning." <i>Nature Machine Intelligence (2024): 1-10</i>.</span></li>
<li><span id="jing2024alphaflow">Jing, Bowen, et al. "AlphaFold meets flow matching for generating protein ensembles." <i>The 41st International Conference on Machine Learning (ICML 2024)</i>.</span></li>
<li><span id="lu2025esmdiff">Lu, Jiarui, et al. "Structure Language Models for Protein Conformation Generation." <i>The Thirteenth International Conference on Learning Representations (ICLR 2025)</i>.</span></li>
<li><span id="lewis2024bioemu">Lewis, Sarah, et al. "Scalable emulation of protein equilibrium ensembles with generative deep learning." <i>bioRxiv (2024): 2024-12</i>.</span></li>
<li><span id="fu2023cgmd">Fu, Xiang, et al. "Simulate time-integrated coarse-grained molecular dynamics with multi-scale graph networks." <i>Transactions on Machine Learning Research (2023)</i>.</span></li>
<li><span id="schreiner2023ito">Schreiner, Mathias, et al. "Implicit transfer operator learning: multiple time-resolution surrogates for molecular dynamics." <i>Advances in Neural Information Processing Systems 36 (2023)</i>.</span></li>
<li><span id="klein2023timewarp">Klein, Leon, et al. "Timewarp: Transferable acceleration of molecular dynamics by learning time-coarsened dynamics." <i>Advances in Neural Information Processing Systems 36 (2023)</i>.</span></li>
<li><span id="jing2024mdgen">Jing, Bowen, et al. "Generative modeling of molecular dynamics trajectories." <i>Advances in Neural Information Processing Systems 37 (2024)</i>.</span></li>

  
<!-- <li><span id="sun2019infograph">Sun, F.-Y., Hoffmann, J., Verma, V., &amp; Tang, J. (2020). Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. <i>ICLR</i>.</span></li>
<li><span id="xu2021end">Xu, M., Wang, W., Luo, S., Shi, C., Bengio, Y., Gomez-Bombarelli, R., &amp; Tang, J. (2021). An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming. <i>ArXiv Preprint ArXiv:2105.07246</i>.</span></li>
<li><span id="shi2021learning">Shi, C., Luo, S., Xu, M., &amp; Tang, J. (2021). Learning gradient fields for molecular conformation generation. <i>ArXiv Preprint ArXiv:2105.03902</i>.</span></li>
<li><span id="xu2021learning">Xu, M., Luo, S., Bengio, Y., Peng, J., &amp; Tang, J. (2021). Learning neural generative dynamics for molecular conformation generation. <i>ArXiv Preprint ArXiv:2102.10240</i>.</span></li>
<li><span id="shi2020graphaf">Shi, C., Xu, M., Zhu, Z., Zhang, W., Zhang, M., &amp; Tang, J. (2020). GraphAF: a flow-based autoregressive model for molecular graph generation. <i>ICLR</i>.</span></li>
<li><span id="shhi2020a">Shi, C., Xu, M., Guo, H., Zhang, M., &amp; Tang, J. (2020). A Graph to Graphs Framework for Retrosynthesis Prediction. <i>ICML</i>.</span></li>
<li><span id="gottipati2020learning">Gottipati, S. K., Sattarov, B., Niu, S., Pathak, Y., Wei, H., Liu, S., Thomas, K. M. J., Blackburn, S., Coley, C. W., Tang, J., &amp; others. (2020). Learning To Navigate The Synthetically Accessible Chemical Space Using Reinforcement Learning. <i>ICML</i>.</span></li>
<li><span id="jin2018junction">Jin, W., Barzilay, R., &amp; Jaakkola, T. (2018). Junction tree variational autoencoder for molecular graph generation. <i>ICML</i>.</span></li>
<li><span id="you2018graph">You, J., Liu, B., Ying, Z., Pande, V., &amp; Leskovec, J. (2018). Graph convolutional policy network for goal-directed molecular graph generation. <i>Advances in Neural Information Processing Systems</i>, 6410–6421.</span></li>
<li><span id="zang2020moflow">Zang, C., &amp; Wang, F. (2020). MoFlow: An Invertible Flow Model for Generating Molecular Graphs. <i>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</i>, 617–626.</span></li>
<li><span id="sun2020graph">Sun, M., Zhao, S., Gilvary, C., Elemento, O., Zhou, J., &amp; Wang, F. (2020). Graph convolutional networks for computational drug development and discovery. <i>Briefings in Bioinformatics</i>, <i>21</i>(3), 919–935.</span></li>
<li><span id="hu2019strategies">Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., &amp; Leskovec, J. (2019). Strategies for Pre-training Graph Neural Networks. <i>ArXiv Preprint ArXiv:1905.12265</i>.</span></li>
<li><span id="gilmer2017neural">Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., &amp; Dahl, G. E. (2017). Neural message passing for quantum chemistry. <i>ArXiv Preprint ArXiv:1704.01212</i>.</span></li>
<li><span id="kipf2016semi">Kipf, T. N., &amp; Welling, M. (2016). Semi-supervised classification with graph convolutional networks. <i>ArXiv Preprint ArXiv:1609.02907</i>.</span></li>
<li><span id="xu2018powerful">Xu, K., Hu, W., Leskovec, J., &amp; Jegelka, S. (2018). How powerful are graph neural networks? <i>ArXiv Preprint ArXiv:1810.00826</i>.</span></li>
<li><span id="jin2017predicting">Jin, W., Coley, C., Barzilay, R., &amp; Jaakkola, T. (2017). Predicting organic reaction outcomes with weisfeiler-lehman network. <i>Advances in Neural Information Processing Systems</i>, 2607–2616.</span></li>
<li><span id="schwaller2019molecular">Schwaller, P., Laino, T., Gaudin, T., Bolgar, P., Hunter, C. A., Bekas, C., &amp; Lee, A. A. (2019). Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. <i>ACS Central Science</i>, <i>5</i>(9), 1572–1583.</span></li>
<li><span id="sacha2020molecule">Sacha, M., Błaż, M., Byrski, P., Włodarczyk-Pruszyński, P., &amp; Jastrzębski, S. (2020). Molecule Edit Graph Attention Network: Modeling Chemical Reactions as Sequences of Graph Edits. <i>ArXiv Preprint ArXiv:2006.15426</i>.</span></li>
<li><span id="dai2019retrosynthesis">Dai, H., Li, C., Coley, C., Dai, B., &amp; Song, L. (2019). Retrosynthesis prediction with conditional graph logic network. <i>Advances in Neural Information Processing Systems</i>, 8872–8882.</span></li>
<li><span id="zhou2020network">Zhou, Y., Hou, Y., Shen, J., Huang, Y., Martin, W., &amp; Cheng, F. (2020). Network-based drug repurposing for novel coronavirus 2019-nCoV/SARS-CoV-2. <i>Cell Discovery</i>, <i>6</i>(1), 1–18.</span></li>
<li><span id="zeng2019deepdr">Zeng, X., Zhu, S., Liu, X., Zhou, Y., Nussinov, R., &amp; Cheng, F. (2019). deepDR: a network-based deep learning approach to in silico drug repositioning. <i>Bioinformatics</i>, <i>35</i>(24), 5191–5198.</span></li>
<li><span id="zhou2020artificial">Zhou, Y., Wang, F., Jian, T., R., N., &amp; Cheng, F. (2020). Artificial Intelligence in Drug Repurposing. <i>The Lancet Digital Health</i>.</span></li>
<li><span id="chen2020idrug">Chen, H., Cheng, F., &amp; Li, J. (2020). iDrug: Integration of drug repositioning and drug-target prediction via cross-network embedding. <i>PLoS Computational Biology</i>, <i>16</i>(7), e1008040.</span></li>
<li><span id="cheng2019network">Cheng, F., Kovács, I. A., &amp; Barabási, A.-L. (2019). Network-based prediction of drug combinations. <i>Nature Communications</i>, <i>10</i>(1), 1–11.</span></li>
<li><span id="cheng2018network">Cheng, F., Desai, R. J., Handy, D. E., Wang, R., Schneeweiss, S., Barabási, A.-L., &amp; Loscalzo, J. (2018). Network-based approach to prediction and population-based validation of in silico drug repurposing. <i>Nature Communications</i>, <i>9</i>(1), 1–12.</span></li>
<li><span id="gysi2020network">Gysi, D. M., Valle, Í. D., Zitnik, M., Ameli, A., Gan, X., Varol, O., Sanchez, H., Baron, R. M., Ghiassian, D., Loscalzo, J., &amp; others. (2020). Network medicine framework for identifying drug repurposing opportunities for covid-19. <i>ArXiv Preprint ArXiv:2004.07229</i>.</span></li>
<li><span id="zhou2020networkb">Zhou, Y., Hou, Y., Shen, J., Kallianpur, A., Zein, J., Culver, D. A., Farha, S., Comhair, S., Fiocchi, C., Gack, M. U., &amp; others. (2020). A Network Medicine Approach to Investigation and Population-based Validation of Disease Manifestations and Drug Repurposing for COVID-19. <i>ChemRxiv</i>.</span></li></ol> -->

    </div>
  </div>
</div>
  </div>
  <div class="footer">
  <div class="container">
    <div class="row">
      <div class="col-12 mt-2 mb-4">
        <div class="footer-inner">
          <h3 class="footer-title">Artificial Intelligence for Protein Design, AAAI 2025 Tutorial</h3>
          <ul>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
  <script type="text/javascript" src="https://deepgraphlearning.github.io/ProteinTutorial_AAAI2025/assets/js/scripts.js""></script>
  
</body>
</html>
